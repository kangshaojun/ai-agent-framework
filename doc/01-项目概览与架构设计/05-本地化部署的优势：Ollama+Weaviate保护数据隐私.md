# æœ¬åœ°åŒ–éƒ¨ç½²çš„ä¼˜åŠ¿ï¼šOllama + Weaviateä¿æŠ¤æ•°æ®éšç§

## å‰è¨€

åœ¨æ•°æ®éšç§æ—¥ç›Šé‡è¦çš„ä»Šå¤©ï¼Œä¼ä¸šå¯¹AIåº”ç”¨çš„æœ¬åœ°åŒ–éƒ¨ç½²éœ€æ±‚è¶Šæ¥è¶Šå¼ºçƒˆã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨æœ¬åœ°åŒ–éƒ¨ç½²çš„ä¼˜åŠ¿ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨Ollamaå’ŒWeaviateæ„å»ºå®Œå…¨ç§æœ‰çš„AIç³»ç»Ÿã€‚

**é€‚åˆè¯»è€…ï¼š** ä¼ä¸šæ¶æ„å¸ˆã€CTOã€å®‰å…¨å·¥ç¨‹å¸ˆã€AIå¼€å‘è€…

---

## ä¸€ã€äº‘ç«¯APIçš„éšç§é£é™©

### 1.1 æ•°æ®æ³„éœ²é£é™©

```
ä¼ä¸šä½¿ç”¨OpenAI APIçš„æ•°æ®æµï¼š

ç”¨æˆ·é—®é¢˜ï¼š"æˆ‘ä»¬å…¬å¸Q3è´¢æŠ¥æ˜¾ç¤º..."
    â†“
é€šè¿‡HTTPSå‘é€åˆ°OpenAIæœåŠ¡å™¨
    â†“
OpenAIæœåŠ¡å™¨å¤„ç†ï¼ˆæ•°æ®å·²ç¦»å¼€ä¼ä¸šï¼‰
    â†“
è¿”å›ç­”æ¡ˆ

é£é™©ï¼š
âŒ æ•æ„Ÿæ•°æ®ä¸Šä¼ åˆ°ç¬¬ä¸‰æ–¹
âŒ æ— æ³•ä¿è¯æ•°æ®ä¸è¢«ç”¨äºè®­ç»ƒ
âŒ æœåŠ¡å•†å¯èƒ½è¢«é»‘å®¢æ”»å‡»
```

### 1.2 æˆæœ¬é—®é¢˜

å¯¹äºé«˜é¢‘ä½¿ç”¨çš„ä¼ä¸šåœºæ™¯ï¼Œäº‘ç«¯APIæŒ‰è°ƒç”¨æ¬¡æ•°è®¡è´¹ï¼Œé•¿æœŸç´¯ç§¯æˆæœ¬éå¸¸é«˜æ˜‚ã€‚è€Œæœ¬åœ°éƒ¨ç½²è™½ç„¶éœ€è¦ä¸€æ¬¡æ€§çš„ç¡¬ä»¶æŠ•å…¥å’Œæ—¥å¸¸è¿ç»´æˆæœ¬ï¼Œä½†ä»é•¿æœŸæ¥çœ‹ï¼Œèƒ½å¤Ÿæ˜¾è‘—é™ä½æ€»ä½“æ‹¥æœ‰æˆæœ¬ï¼ˆTCOï¼‰ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§è§„æ¨¡ã€é«˜å¹¶å‘çš„åº”ç”¨åœºæ™¯ï¼Œæˆæœ¬ä¼˜åŠ¿æ›´åŠ æ˜æ˜¾ã€‚

---

## äºŒã€æœ¬åœ°åŒ–éƒ¨ç½²æ¶æ„

### 2.1 å®Œæ•´æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ä¼ä¸šå†…ç½‘ç¯å¢ƒ                         â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Frontend   â”‚      â”‚    Server    â”‚        â”‚
â”‚  â”‚  (Next.js)   â”‚â—„â”€â”€â”€â”€â–ºâ”‚  (FastAPI)   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                               â”‚                 â”‚
â”‚                               â–¼                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚   Ollama     â”‚â—„â”€â”€â”€â”€â–ºâ”‚   Weaviate   â”‚        â”‚
â”‚  â”‚  (LLMæ¨ç†)   â”‚      â”‚  (å‘é‡æ•°æ®åº“) â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                 â”‚
â”‚  æ•°æ®æµï¼š                                        â”‚
â”‚  ç”¨æˆ·é—®é¢˜ â†’ å‘é‡åŒ– â†’ æ£€ç´¢ â†’ LLM â†’ ç­”æ¡ˆ           â”‚
â”‚  âœ… æ‰€æœ‰æ•°æ®éƒ½åœ¨ä¼ä¸šå†…ç½‘                         â”‚
â”‚  âœ… ä¸ç»è¿‡ä»»ä½•ç¬¬ä¸‰æ–¹æœåŠ¡å™¨                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ•°æ®éš”ç¦»

```
ç‰©ç†éš”ç¦»ï¼š
- éƒ¨ç½²åœ¨ä¼ä¸šè‡ªæœ‰æœåŠ¡å™¨
- ä¸è¿æ¥å…¬ç½‘ï¼ˆå¯é€‰ï¼‰
- ä¸“ç”¨ç½‘ç»œç¯å¢ƒ

é€»è¾‘éš”ç¦»ï¼š
- å¤šç§Ÿæˆ·æ•°æ®éš”ç¦»
- åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶
- æ•°æ®åŠ å¯†å­˜å‚¨
```

---

## ä¸‰ã€Ollamaï¼šæœ¬åœ°LLMéƒ¨ç½²

### 3.1 Ollamaç®€ä»‹

```
Ollamaæ˜¯ä»€ä¹ˆï¼Ÿ
- æœ¬åœ°å¤§æ¨¡å‹è¿è¡Œå·¥å…·
- ç±»ä¼¼Dockerï¼Œä½†ä¸“ä¸ºLLMè®¾è®¡
- ä¸€é”®ä¸‹è½½å’Œè¿è¡Œæ¨¡å‹

æ”¯æŒçš„æ¨¡å‹ï¼š
- Llama 2/3 (Meta)
- Qwen 2.5 (é˜¿é‡Œ)
- Mistral (Mistral AI)
- Gemma (Google)
- 100+ å¼€æºæ¨¡å‹
```

### 3.2 å®‰è£…å’Œä½¿ç”¨

```bash
# 1. å®‰è£…Ollama (macOS/Linux)
curl -fsSL https://ollama.ai/install.sh | sh

# 2. ä¸‹è½½æ¨¡å‹
ollama pull llama3.2:latest      # å¯¹è¯æ¨¡å‹
ollama pull nomic-embed-text     # Embeddingæ¨¡å‹

# 3. å¯åŠ¨æœåŠ¡
ollama serve  # ç›‘å¬ http://localhost:11434

# 4. æµ‹è¯•
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.2:latest",
  "prompt": "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
}'
```

### 3.3 Pythoné›†æˆ

```python
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings

# 1. åˆå§‹åŒ–å¯¹è¯æ¨¡å‹
llm = Ollama(
    model="llama3.2:latest",
    base_url="http://localhost:11434",
    temperature=0.7,
    num_ctx=4096  # ä¸Šä¸‹æ–‡é•¿åº¦
)

# 2. åŒæ­¥è°ƒç”¨
response = llm.invoke("ä»€ä¹ˆæ˜¯RAGï¼Ÿ")
print(response)

# 3. æµå¼è°ƒç”¨
for chunk in llm.stream("è®²ä¸ªç¬‘è¯"):
    print(chunk, end="", flush=True)

# 4. å¼‚æ­¥æµå¼è°ƒç”¨
async for chunk in llm.astream("å†™ä¸€é¦–è¯—"):
    print(chunk, end="", flush=True)

# 5. Embeddingæ¨¡å‹
embeddings = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://localhost:11434"
)

# ç”Ÿæˆå‘é‡
vector = embeddings.embed_query("è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬")
print(f"å‘é‡ç»´åº¦: {len(vector)}")  # 768ç»´
```

### 3.4 æ¨¡å‹é€‰æ‹©

```python
# ä¸åŒè§„æ¨¡æ¨¡å‹å¯¹æ¯”
models = {
    "llama3.2:1b": {
        "å‚æ•°é‡": "1B",
        "æ˜¾å­˜éœ€æ±‚": "2GB",
        "é€Ÿåº¦": "â­â­â­â­â­",
        "è´¨é‡": "â­â­â­",
        "é€‚ç”¨åœºæ™¯": "ç®€å•é—®ç­”ã€èµ„æºå—é™"
    },
    "llama3.2:3b": {
        "å‚æ•°é‡": "3B",
        "æ˜¾å­˜éœ€æ±‚": "4GB",
        "é€Ÿåº¦": "â­â­â­â­",
        "è´¨é‡": "â­â­â­â­",
        "é€‚ç”¨åœºæ™¯": "é€šç”¨å¯¹è¯ã€ä¼ä¸šåº”ç”¨"
    },
    "llama3.2:latest": {
        "å‚æ•°é‡": "3B",
        "æ˜¾å­˜éœ€æ±‚": "4GB",
        "é€Ÿåº¦": "â­â­â­â­",
        "è´¨é‡": "â­â­â­â­",
        "é€‚ç”¨åœºæ™¯": "é€šç”¨å¯¹è¯ã€ä¼ä¸šåº”ç”¨ï¼ˆæ¨èï¼‰"
    }
}

# æ¨èé…ç½®
# å¼€å‘ç¯å¢ƒ: llama3.2:latest (4GBæ˜¾å­˜)
# ç”Ÿäº§ç¯å¢ƒ: llama3.2:latest (4GBæ˜¾å­˜)
# è¾¹ç¼˜è®¾å¤‡: llama3.2:1b (2GBæ˜¾å­˜)
```

### 3.5 æ€§èƒ½ä¼˜åŒ–

```bash
# 1. GPUåŠ é€Ÿ
# è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨GPU
ollama serve

# 2. é‡åŒ–æ¨¡å‹ï¼ˆå‡å°‘æ˜¾å­˜å ç”¨ï¼‰
ollama pull llama3.2:latest  # å·²ä¼˜åŒ–ç‰ˆæœ¬

# 3. å¹¶å‘é…ç½®
export OLLAMA_NUM_PARALLEL=4  # æ”¯æŒ4ä¸ªå¹¶å‘è¯·æ±‚
export OLLAMA_MAX_LOADED_MODELS=2  # æœ€å¤šåŠ è½½2ä¸ªæ¨¡å‹

# 4. ä¸Šä¸‹æ–‡é•¿åº¦
export OLLAMA_NUM_CTX=8192  # 8Kä¸Šä¸‹æ–‡
```

---

## å››ã€Weaviateï¼šæœ¬åœ°å‘é‡æ•°æ®åº“

### 4.1 Weaviateç®€ä»‹

```
Weaviateæ˜¯ä»€ä¹ˆï¼Ÿ
- å¼€æºå‘é‡æ•°æ®åº“
- æ”¯æŒè¯­ä¹‰æœç´¢
- å†…ç½®å‘é‡åŒ–åŠŸèƒ½
- GraphQLæŸ¥è¯¢

æ ¸å¿ƒç‰¹æ€§ï¼š
âœ… é«˜æ€§èƒ½å‘é‡æ£€ç´¢
âœ… æ··åˆæœç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰
âœ… å¤šç§Ÿæˆ·æ”¯æŒ
âœ… æ°´å¹³æ‰©å±•
```

### 4.2 Dockeréƒ¨ç½²

```yaml
# docker-compose.yml
version: '3.8'

services:
  weaviate:
    image: semitechnologies/weaviate:1.27.1
    ports:
      - "8080:8080"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none'  # ä½¿ç”¨å¤–éƒ¨Embedding
      CLUSTER_HOSTNAME: 'node1'
    volumes:
      - weaviate_data:/var/lib/weaviate

volumes:
  weaviate_data:
```

```bash
# å¯åŠ¨Weaviate
docker-compose up -d

# æ£€æŸ¥çŠ¶æ€
curl http://localhost:8080/v1/meta
```

### 4.3 åˆ›å»ºSchema

```python
import weaviate

# è¿æ¥Weaviate
client = weaviate.Client("http://localhost:8080")

# åˆ›å»ºSchema
schema = {
    "class": "ServiceTicket",
    "description": "å®¢æœå·¥å•çŸ¥è¯†åº“",
    "vectorizer": "none",  # ä½¿ç”¨å¤–éƒ¨Embedding
    "properties": [
        {
            "name": "ticket_id",
            "dataType": ["string"],
            "description": "å·¥å•ID"
        },
        {
            "name": "title",
            "dataType": ["text"],
            "description": "å·¥å•æ ‡é¢˜"
        },
        {
            "name": "description",
            "dataType": ["text"],
            "description": "é—®é¢˜æè¿°"
        },
        {
            "name": "solution",
            "dataType": ["text"],
            "description": "è§£å†³æ–¹æ¡ˆ"
        },
        {
            "name": "category",
            "dataType": ["string"],
            "description": "åˆ†ç±»"
        },
        {
            "name": "content",
            "dataType": ["text"],
            "description": "å®Œæ•´å†…å®¹ï¼ˆç”¨äºå‘é‡åŒ–ï¼‰"
        }
    ]
}

# åˆ›å»ºCollection
client.schema.create_class(schema)
```

### 4.4 æ•°æ®å¯¼å…¥

```python
import pandas as pd
from langchain_community.embeddings import OllamaEmbeddings

# 1. è¯»å–CSV
df = pd.read_csv("service_tickets.csv")

# 2. æ•°æ®æ¸…æ´—
df = df.dropna()
df = df.drop_duplicates()

# 3. ç»„åˆæ–‡æœ¬
df['content'] = (
    "å·¥å•ID: " + df['ticket_id'].astype(str) + "\n" +
    "æ ‡é¢˜: " + df['title'] + "\n" +
    "æè¿°: " + df['description'] + "\n" +
    "è§£å†³æ–¹æ¡ˆ: " + df['solution']
)

# 4. åˆå§‹åŒ–Embedding
embeddings = OllamaEmbeddings(
    model="nomic-embed-text",
    base_url="http://localhost:11434"
)

# 5. æ‰¹é‡å¯¼å…¥
batch_size = 100
for i in range(0, len(df), batch_size):
    batch = df[i:i+batch_size]
    
    # ç”Ÿæˆå‘é‡
    texts = batch['content'].tolist()
    vectors = embeddings.embed_documents(texts)
    
    # å¯¼å…¥Weaviate
    with client.batch as batch_obj:
        for idx, row in batch.iterrows():
            properties = {
                "ticket_id": row['ticket_id'],
                "title": row['title'],
                "description": row['description'],
                "solution": row['solution'],
                "category": row['category'],
                "content": row['content']
            }
            
            batch_obj.add_data_object(
                properties,
                "ServiceTicket",
                vector=vectors[idx - i]
            )
    
    print(f"å·²å¯¼å…¥ {i+len(batch)}/{len(df)} æ¡æ•°æ®")
```

### 4.5 å‘é‡æ£€ç´¢

```python
from langchain_community.vectorstores import Weaviate

# åˆå§‹åŒ–å‘é‡å­˜å‚¨
vectorstore = Weaviate(
    client=client,
    index_name="ServiceTicket",
    text_key="content",
    embedding=embeddings
)

# 1. ç›¸ä¼¼åº¦æœç´¢
docs = vectorstore.similarity_search(
    "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ",
    k=5  # è¿”å›Top-5
)

for doc in docs:
    print(f"æ ‡é¢˜: {doc.metadata['title']}")
    print(f"å†…å®¹: {doc.page_content[:100]}...")
    print("---")

# 2. å¸¦åˆ†æ•°çš„æœç´¢
docs_with_scores = vectorstore.similarity_search_with_score(
    "å¦‚ä½•é‡ç½®å¯†ç ï¼Ÿ",
    k=5
)

for doc, score in docs_with_scores:
    print(f"ç›¸ä¼¼åº¦: {score:.4f}")
    print(f"æ ‡é¢˜: {doc.metadata['title']}")
    print("---")

# 3. æ··åˆæœç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰
docs = vectorstore.similarity_search(
    "é‡ç½®å¯†ç ",
    search_type="hybrid",  # æ··åˆæœç´¢
    k=5
)

# 4. è¿‡æ»¤æœç´¢
docs = vectorstore.similarity_search(
    "è´¦å·é—®é¢˜",
    k=5,
    where_filter={
        "path": ["category"],
        "operator": "Equal",
        "valueString": "è´¦å·ç®¡ç†"
    }
)
```

---

## äº”ã€å®Œæ•´RAGå®ç°

### 5.1 RAGå¼•æ“

```python
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA

class RAGEngine:
    def __init__(self):
        # 1. Embeddingæ¨¡å‹
        self.embeddings = OllamaEmbeddings(
            model="nomic-embed-text",
            base_url="http://localhost:11434"
        )
        
        # 2. å‘é‡æ•°æ®åº“
        self.vectorstore = Weaviate(
            client=weaviate_client,
            index_name="ServiceTicket",
            text_key="content",
            embedding=self.embeddings
        )
        
        # 3. LLM
        self.llm = Ollama(
            model="llama3.2:latest",
            base_url="http://localhost:11434",
            temperature=0.7
        )
        
        # 4. Promptæ¨¡æ¿
        self.prompt = PromptTemplate(
            template="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¦æ±‚ï¼š
1. å¦‚æœä¸Šä¸‹æ–‡ä¸­æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯¦ç»†å›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·è¯šå®è¯´æ˜
3. å›ç­”è¦ä¸“ä¸šã€å‹å¥½ã€ç®€æ´

å›ç­”ï¼š""",
            input_variables=["context", "question"]
        )
    
    def search(self, query: str, k: int = 5):
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        return self.vectorstore.similarity_search(query, k=k)
    
    def answer(self, question: str):
        """ç”Ÿæˆç­”æ¡ˆ"""
        # 1. æ£€ç´¢
        docs = self.search(question)
        
        # 2. ç»„è£…ä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"æ–‡æ¡£{i+1}:\n{doc.page_content}"
            for i, doc in enumerate(docs)
        ])
        
        # 3. ç”Ÿæˆç­”æ¡ˆ
        prompt_text = self.prompt.format(
            context=context,
            question=question
        )
        
        return self.llm.invoke(prompt_text)
    
    async def astream_answer(self, question: str):
        """æµå¼ç”Ÿæˆç­”æ¡ˆ"""
        # 1. æ£€ç´¢
        docs = self.search(question)
        
        # 2. ç»„è£…ä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"æ–‡æ¡£{i+1}:\n{doc.page_content}"
            for i, doc in enumerate(docs)
        ])
        
        # 3. æµå¼ç”Ÿæˆ
        prompt_text = self.prompt.format(
            context=context,
            question=question
        )
        
        async for chunk in self.llm.astream(prompt_text):
            yield chunk
```

### 5.2 HTTPæœåŠ¡

```python
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
import json

app = FastAPI()
rag_engine = RAGEngine()

@app.post("/chat/stream")
async def chat_stream(question: str):
    """æµå¼é—®ç­”æ¥å£"""
    
    async def event_generator():
        try:
            # 1. æ€è€ƒçŠ¶æ€
            yield format_sse("thinking", {"status": "retrieving"})
            
            # 2. æ£€ç´¢æ–‡æ¡£
            docs = rag_engine.search(question)
            yield format_sse("sources", {
                "count": len(docs),
                "sources": [
                    {
                        "title": doc.metadata.get("title", ""),
                        "category": doc.metadata.get("category", "")
                    }
                    for doc in docs
                ]
            })
            
            # 3. æµå¼ç”Ÿæˆç­”æ¡ˆ
            async for chunk in rag_engine.astream_answer(question):
                yield format_sse("token", {"token": chunk})
            
            # 4. å®Œæˆ
            yield format_sse("done", {"status": "completed"})
            
        except Exception as e:
            yield format_sse("error", {"error": str(e)})
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"
    )

def format_sse(event: str, data: dict) -> str:
    return f"event: {event}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n"
```

---

## å…­ã€æ•°æ®å®‰å…¨ä¿éšœ

### 6.1 ç½‘ç»œéš”ç¦»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         ä¼ä¸šå†…ç½‘ï¼ˆ192.168.1.0/24ï¼‰    â”‚
â”‚                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Frontend â”‚      â”‚  Server  â”‚   â”‚
â”‚  â”‚ å†…ç½‘è®¿é—®  â”‚â—„â”€â”€â”€â”€â–ºâ”‚  å†…ç½‘è®¿é—® â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                          â”‚         â”‚
â”‚                          â–¼         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Ollama  â”‚â—„â”€â”€â”€â”€â–ºâ”‚ Weaviate â”‚   â”‚
â”‚  â”‚ ä»…å†…ç½‘è®¿é—®â”‚      â”‚ ä»…å†…ç½‘è®¿é—®â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²
         â”‚ é˜²ç«å¢™
         â”‚ âŒ ç¦æ­¢å¤–ç½‘è®¿é—®
         â–¼
    Internet
```

### 6.2 è®¿é—®æ§åˆ¶

```python
# JWTè®¤è¯
from fastapi import Depends, HTTPException
from jose import jwt

async def verify_token(token: str = Depends(oauth2_scheme)):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=["HS256"])
        return payload
    except:
        raise HTTPException(status_code=401, detail="æ— æ•ˆçš„Token")

@app.post("/chat/stream")
async def chat_stream(
    question: str,
    user: dict = Depends(verify_token)  # éœ€è¦è®¤è¯
):
    # åªæœ‰è®¤è¯ç”¨æˆ·æ‰èƒ½è®¿é—®
    pass
```

### 6.3 æ•°æ®åŠ å¯†

```python
# æ•æ„Ÿæ•°æ®åŠ å¯†å­˜å‚¨
from cryptography.fernet import Fernet

# ç”Ÿæˆå¯†é’¥
key = Fernet.generate_key()
cipher = Fernet(key)

# åŠ å¯†
encrypted_data = cipher.encrypt(b"æ•æ„Ÿä¿¡æ¯")

# è§£å¯†
decrypted_data = cipher.decrypt(encrypted_data)
```

### 6.4 å®¡è®¡æ—¥å¿—

```python
from loguru import logger

# é…ç½®æ—¥å¿—
logger.add(
    "logs/audit_{time}.log",
    rotation="1 day",
    retention="30 days",
    format="{time} | {level} | {extra[user_id]} | {message}"
)

@app.post("/chat/stream")
async def chat_stream(
    question: str,
    user: dict = Depends(verify_token)
):
    # è®°å½•å®¡è®¡æ—¥å¿—
    logger.bind(user_id=user['user_id']).info(
        f"ç”¨æˆ·æé—®: {question[:50]}..."
    )
    
    # å¤„ç†è¯·æ±‚
    pass
```

---

## ä¸ƒã€ç¡¬ä»¶é…ç½®å»ºè®®

### 7.1 å¼€å‘ç¯å¢ƒ

```
CPU: 8æ ¸å¿ƒä»¥ä¸Š
å†…å­˜: 16GB
GPU: NVIDIA RTX 3060 (12GBæ˜¾å­˜)
å­˜å‚¨: 500GB SSD

æˆæœ¬: ~$1,500

æ”¯æŒ:
- llama3.2:latest æ¨¡å‹
- 10ä¸‡æ¡æ–‡æ¡£å‘é‡åŒ–
- 10ä¸ªå¹¶å‘ç”¨æˆ·
```

### 7.2 ç”Ÿäº§ç¯å¢ƒ

```
CPU: 32æ ¸å¿ƒ
å†…å­˜: 128GB
GPU: NVIDIA A100 (80GBæ˜¾å­˜) Ã— 2
å­˜å‚¨: 2TB NVMe SSD

æˆæœ¬: ~$20,000

æ”¯æŒ:
- llama3.2:latest æ¨¡å‹ï¼ˆå¤šå®ä¾‹ï¼‰
- 1000ä¸‡æ¡æ–‡æ¡£å‘é‡åŒ–
- 1000ä¸ªå¹¶å‘ç”¨æˆ·
```

### 7.3 è¾¹ç¼˜éƒ¨ç½²

```
è®¾å¤‡: NVIDIA Jetson AGX Orin
CPU: 12æ ¸å¿ƒ ARM
å†…å­˜: 32GB
GPU: 2048 CUDAæ ¸å¿ƒ
å­˜å‚¨: 256GB SSD

æˆæœ¬: ~$2,000

æ”¯æŒ:
- llama3.2:1b æ¨¡å‹
- 10ä¸‡æ¡æ–‡æ¡£å‘é‡åŒ–
- 50ä¸ªå¹¶å‘ç”¨æˆ·
```

---

## å…«ã€æˆæœ¬å¯¹æ¯”åˆ†æ

### 8.1 3å¹´TCOå¯¹æ¯”

| é¡¹ç›® | äº‘ç«¯API | æœ¬åœ°éƒ¨ç½² |
|------|---------|----------|
| **åˆå§‹æŠ•èµ„** | $0 | $20,000 |
| **å¹´åº¦APIè´¹ç”¨** | $3,240,000 | $0 |
| **å¹´åº¦ç”µè´¹** | $0 | $1,200 |
| **å¹´åº¦ç»´æŠ¤** | $0 | $5,000 |
| **3å¹´æ€»æˆæœ¬** | $9,720,000 | $38,600 |
| **èŠ‚çœ** | - | $9,681,400 (99.6%) |

### 8.2 ROIåˆ†æ

```
æŠ•èµ„å›æŠ¥å‘¨æœŸ: 2.3å¤©
å¹´åº¦ROI: 15,800%
3å¹´ROI: 25,000%
```

---

## ä¹ã€è¸©å‘ç»éªŒ

### 9.1 æ˜¾å­˜ä¸è¶³

âŒ **é—®é¢˜ï¼š** æ¨¡å‹åŠ è½½å¤±è´¥

```bash
Error: CUDA out of memory
```

âœ… **è§£å†³ï¼š** ä½¿ç”¨é‡åŒ–æ¨¡å‹

```bash
# ä½¿ç”¨ä¼˜åŒ–æ¨¡å‹
ollama pull llama3.2:latest

# æˆ–å‡å°ä¸Šä¸‹æ–‡é•¿åº¦
export OLLAMA_NUM_CTX=2048
```

### 9.2 Weaviateæ€§èƒ½

âŒ **é—®é¢˜ï¼š** æ£€ç´¢é€Ÿåº¦æ…¢

âœ… **è§£å†³ï¼š** åˆ›å»ºç´¢å¼•

```python
# åˆ›å»ºHNSWç´¢å¼•
schema = {
    "class": "ServiceTicket",
    "vectorIndexConfig": {
        "ef": 200,  # æé«˜å¬å›ç‡
        "efConstruction": 256,
        "maxConnections": 64
    }
}
```

### 9.3 å†…å­˜æ³„æ¼

âŒ **é—®é¢˜ï¼š** é•¿æ—¶é—´è¿è¡Œåå†…å­˜å ç”¨é«˜

âœ… **è§£å†³ï¼š** å®šæœŸæ¸…ç†

```python
import gc

# å®šæœŸæ¸…ç†
gc.collect()

# å¸è½½æ¨¡å‹
ollama stop llama3.2:latest
```

---

## åã€æ€»ç»“

æœ¬åœ°åŒ–éƒ¨ç½²çš„æ ¸å¿ƒä¼˜åŠ¿ï¼š

âœ… **æ•°æ®éšç§** - æ•°æ®ä¸ç¦»å¼€ä¼ä¸šå†…ç½‘  
âœ… **æˆæœ¬èŠ‚çœ** - 3å¹´èŠ‚çœ99.6%æˆæœ¬  
âœ… **åˆè§„æ€§** - æ»¡è¶³å„ç±»æ•°æ®ä¿æŠ¤æ³•è§„  
âœ… **å¯æ§æ€§** - å®Œå…¨è‡ªä¸»å¯æ§  
âœ… **å®šåˆ¶åŒ–** - å¯å¾®è°ƒæ¨¡å‹  

**ä¸‹ä¸€ç¯‡é¢„å‘Šï¼š** ã€ŠNext.js 13æ„å»ºç°ä»£åŒ–AIèŠå¤©ç•Œé¢ã€‹

---

**ä½œè€…ç®€ä»‹ï¼š** èµ„æ·±å¼€å‘è€…ï¼Œåˆ›ä¸šè€…ã€‚ä¸“æ³¨äºè§†é¢‘é€šè®¯æŠ€æœ¯é¢†åŸŸã€‚å›½å†…é¦–æœ¬Flutterè‘—ä½œã€ŠFlutteræŠ€æœ¯å…¥é—¨ä¸å®æˆ˜ã€‹ä½œè€…,å¦è‘—æœ‰ã€ŠDartè¯­è¨€å®æˆ˜ã€‹åŠã€ŠWebRTCéŸ³è§†é¢‘å¼€å‘ã€‹ç­‰ä¹¦ç±ã€‚å¤šå¹´ä»äº‹è§†é¢‘ä¼šè®®ã€è¿œç¨‹æ•™è‚²ç­‰æŠ€æœ¯ç ”å‘ï¼Œå¯¹äºAndroidã€iOSä»¥åŠè·¨å¹³å°å¼€å‘æŠ€æœ¯æœ‰æ¯”è¾ƒæ·±å…¥çš„ç ”ç©¶å’Œåº”ç”¨ï¼Œä½œä¸ºä¸»è¦ç¨‹åºå‘˜å¼€å‘äº†å¤šä¸ªåº”ç”¨é¡¹ç›®ï¼Œæ¶‰åŠåŒ»ç–—ã€äº¤é€šã€é“¶è¡Œç­‰é¢†åŸŸã€‚

**å­¦ä¹ èµ„æ–™ï¼š**
- [é¡¹ç›®åœ°å€](https://github.com/kangshaojun/ai-agent-framework)
- [ä½œè€…GitHub](https://github.com/kangshaojun)

**æ¬¢è¿äº¤æµï¼š** å¦‚æœ‰é—®é¢˜æ¬¢è¿åœ¨è¯„è®ºåŒºè®¨è®º ğŸš€
